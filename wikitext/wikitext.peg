{
package wikitext

}

/*********************************************************
 * The top-level rule
 *********************************************************/

start <- tlb:tlb* newlineToken* {
  n := &html.Node{
    Type: html.DocumentNode,
  }
  addChild(n, tlb)
  if len(c.state) > 0 {
    panic(errors.Errorf("poluted state! %#v", c.state))
  }
  return n, nil
}

/*
 * Redirects can only occur as the first thing in a document.  See
 * WikitextContent::getRedirectTarget()
 */
redirect <- redirect_word
    space_or_newline*
    (":" space_or_newline*)?
    wl:wikilink & {
    /*
      return wl.length === 1 && wl[0] && wl[0].constructor !== String;
      */
      return false, nil
  } {
  /*
    var link = wl[0];
    if (sp) { rw += sp; }
    if (c) { rw += c; }
// Build a redirect token
    var redirect = new SelfclosingTagTk('mw:redirect',
// Put 'href' into attributes so it gets template-expanded
            [Util.lookupKV(link.attribs, 'href')],
            {
                src: rw,
                tsr: tsrOffsets(),
                linkTk: link,
            });
    return redirect;
    */
    return "todo redirect", nil
}

// These rules are exposed as start rules.
generic_newline_attributes <- generic_newline_attribute*

table_attributes
  <- (table_attribute / optionalSpaceToken b:broken_table_attribute_name_char {
  return b, nil })*

//The 'redirect' magic word.
// The leading whitespace allowed is due to the PHP trim() function.

redirect_word
  <- ([ \t\n\r]*
    (!space_or_newline ![:[] .)+
    & {return false, nil /*return env.conf.wiki.getMagicWordMatcher('redirect').test(rw);*/ })


//# This rule exists to support tokenizing the document in chunks.
//# The parser's streaming interface will stop tokenization after each iteration
//# of the starred subexpression, and yield to the node.js event-loop to
//# schedule other pending event handlers.
//#
start_async
  <- (tlb
    / newlineToken* &{
    return false, nil
    /*
      if (endOffset() === input.length) {
          emitChunk([ new EOFTk() ]);
      }
// terminate the loop
      return false;
    */
    }
    )*


// A document (start rule) is a sequence of toplevelblocks. Tokens are
// emitted in chunks per toplevelblock to avoid buffering the full document.
//
tlb <- !eof b:block {
  return b, nil
}


// The actual contents of each block.
//
block
      // has to be first alternative; otherwise gets parsed as a <ol>
    <- &sof redirect comment_or_includes block_line? {return "comment_or_includes", nil  /*return [r].concat(cil, bl || []);*/ }
    / block_lines
    / & '<' rs:( cm:comment &eolf {return cm, nil  /*return c;*/ }
           // avoid a paragraph if we know that the line starts with a block tag
           / block_tag
           ) {return rs, nil  /*return rs;*/ }
    / paragraph
    // Inlineline includes generic tags; wrapped into paragraphs in token
    // transform and DOM postprocessor
    / inlineline
    / s:sol !inline_breaks {return s, nil /*return s;*/ }


// A block nested in other constructs. Avoid eating end delimiters for other
// constructs by checking against inline_breaks first.
//
nested_block <- !inline_breaks b:block {return b, nil /*return b;*/ }


// The same, but suitable for use inside a table construct.
// Doesn't match table_heading_tag, table_row_tag, table_data_tag,
// table_caption tag, or table_end_tag, although it does allow
// table_start_tag (for nested tables).
//
nested_block_in_table
  <-
    // avoid recursion via nested_block_in_table, as that can lead to stack
    // overflow in large tables
    // See https://phabricator.wikimedia.org/T59670
    #{
    push(c, "tableDataBlock", true)
    return nil
    /*
    return stops.push('tableDataBlock', true);
    */
    }
    // XXX: don't rely on a lame look-ahead like this; use syntax stops
    // instead, so that multi-line th content followed by a line prefixed with
    // a comment is also handled. Alternatively, implement a sol look-behind
    // assertion accepting spaces and comments.
    !(sol (space* sol)? space* (pipe / "!")) b:nested_block
    #{pop(c, "tableDataBlock"); return nil}
    {
    return b, nil
    /*
        stops.pop('tableDataBlock');
        return b;
        */
    }


// Line-based block constructs.
//
block_lines
  <- s:sol
    // eat an empty line before the block
    (s2:(os:optionalSpaceToken so:sol))?
    bl:block_line

// Horizontal rules
hr <- "----" "-"*
// Check if a newline or content follows
  ( &sol "" {return nil, nil /*return undefined;*/ } / "" {return true, nil /*return true;*/ } ) {
  return &html.Node{
    Type: html.ElementNode,
    Data: "hr",
  }, nil
  /*
    var dataAttribs = {
      tsr: tsrOffsets(),
      lineContent: lineContent,
    };
    if (d.length > 0) {
      dataAttribs.extra_dashes = d.length;
    }
    return new SelfclosingTagTk('hr', [], dataAttribs);
    */
  }


// Block structures with start-of-line wiki syntax
//
block_line
  <- heading
  / list_item
  / hr
  / st: space_or_newline*
    r:( & [ <{}|!] tl:table_line {return tl, nil /*return tl;*/ }
// tag-only lines should not trigger pre either
      / bts:(bt:block_tag stl:optionalSpaceToken {return concat(bt, stl), nil /*return bt.concat(stl);*/ })+
        &eolf {return bts, nil /*return bts;*/ }
      ) {return concat(st, r), nil
      /*
          return st.concat(r);
          */
      }


// A paragraph. We don't emit 'p' tokens to avoid issues with template
// transclusions, <p> tags in the source and the like. Instead, we perform
// some paragraph wrapping on the token stream and the DOM.
//
paragraph
  <- s1:sol s2:sol c1:inlineline {
  n := &html.Node{
    Type: html.ElementNode,
    Data: "p",
  }
  addChild(n, c1)
  return n, nil
}

br <- optionalSpaceToken &newline {
  return &html.Node{
    Type: html.ElementNode,
    Data: "br",
  }, nil
/*
    return s.concat([
      new SelfclosingTagTk('br', [], { tsr: tsrOffsets() }),
    ]);
    */
}

inline_breaks <- & { return inlineBreaks(c) }

inlineline
  <- ((r:urltext #{log.Printf("urltext %q", concat(r)); return nil})
    / inlineline_element)+

inlineline_element
  <- !inline_breaks
    r:(inline_element / [^\r\n])
    #{log.Printf("inline_element: %q", concat(r)); return nil}
    {return r, nil}

inline_element
  <- & '<' r:( xmlish_tag
          / comment
          ) {return r, nil /*return r;*/ }
    / & '{' r:tplarg_or_template {return r, nil/* return r; */}
    / & "-{" r:lang_variant_or_tpl {return r, nil/* return r; */}
// FIXME: The php parser's replaceInternalLinks2 splits on [[, resulting
// in sequences with odd number of brackets parsing as text, and sequences
// with even number of brackets having its innermost pair parse as a
// wikilink.  For now, we faithfully reproduce what's found there but
// wikitext, the language, shouldn't be defined by odd tokenizing behaviour
// in the php parser.  Flagging this for a future cleanup.
    / ("[[" &'[')+
    / & '[' r:( wikilink / extlink ) {return r, nil/* return r; */}
    / & "'" r:quote {return r, nil/* return r; */}

// Headings  */

heading <- & "=" // guard, to make sure '='+ will match.
// XXX: Also check to end to avoid inline parsing?
    r:(
     #{ inc(c, "h"); return nil /*return stops.inc('h');*/ }
     s:'='+ // moved in here to make s accessible to inner action
     ce:(
       (ill:(inlineline?))
       '='+ {return ill, nil}
     )?
     & {
       return ce!=nil || len(concat(s)) > 2, nil
       /*return ce || s.length > 2;*/
     }
     //("" {return nil, nil /*return endOffset();*/ })
     spc:(spaces / comment)*
     &eolf
     #{dec(c, "h"); return nil}
     {
     n := &html.Node{
       Type: html.ElementNode,
       Data: "h"+strconv.Itoa(len(concat(s))),
     }
     addChild(n, []interface{}{ce, spc})
     return n, nil
     /*
        var c;
        var e;
        var level;
        stops.dec('h');
        if (ce) {
            c = ce[0];
            e = ce[1];
            level = Math.min(s.length, e.length);
        } else {
// split up equal signs into two equal parts, with at least
// one character in the middle.
            level = Math.floor((s.length - 1) / 2);
            c = ['='.repeat(s.length - 2 * level)];
            s = e = '='.repeat(level);
        }
        level = Math.min(6, level);
// convert surplus equals into text
        if (s.length > level) {
            var extras1 = s.substr(0, s.length - level);
            if (c[0].constructor === String) {
                c[0] = extras1 + c[0];
            } else {
                c.unshift(extras1);
            }
        }
        if (e.length > level) {
            var extras2 = e.substr(0, e.length - level);
            var lastElem = lastItem(c);
            if (lastElem.constructor === String) {
                c[c.length - 1] += extras2;
            } else {
                c.push(extras2);
            }
        }

        var tsr = tsrOffsets('start');
        tsr[1] += level;
        return [
          new TagTk('h' + level, [], { tsr: tsr }),
        ].concat(c, [
          new EndTagTk('h' + level, [], { tsr: [endTPos - level, endTPos] }),
          spc,
        ]);
        */
      }
    ) {
    return r, nil /*return r;*/
  }


// Comments */

// The php parser does a straight str.replace(/<!--((?!-->).)*-->/g, "")
// but, as always, things around here are a little more complicated.
//
// We accept the same comments, but because we emit them as HTML comments
// instead of deleting them, we have to encode the data to ensure that
// we always emit a valid HTML5 comment.  See the encodeComment helper
// for further details.

comment
    <- "<!--" c1:(!"-->" .)* ("-->" / eof) {
      return &html.Node{
      Type: html.CommentNode,
      Data: concat(c1),
      }, nil
    /*
        var data = DU.encodeComment(c);
        return [new CommentTk(data, { tsr: tsrOffsets() })];
        */
    }


// Behavior switches. See:
// https://www.mediawiki.org/wiki/Help:Magic_words#Behavior_switches
behavior_switch
  <- ("__" behavior_text "__") {return "behavior_text", nil
  /*
    if (env.conf.wiki.isMagicWord(bs)) {
      return [
        new SelfclosingTagTk('behavior-switch', [ new KV('word', bs) ],
          { tsr: tsrOffsets(), src: bs, magicSrc: bs }
        ),
      ];
    } else {
      return [ bs ];
    }
    */
  }

// Instead of defining a charset, php's doDoubleUnderscore concats a regexp of
// all the language specific aliases of the behavior switches and then does a
// match and replace. Just be as permissive as possible and let the
// BehaviorSwitchPreprocessor back out of any overreach.
behavior_text <- ( !"__" [^'"<~[{\n\r:;\]}|!=] )+


// ************************************************************
// External (bracketed and autolinked) links
// ************************************************************/

autolink
  <- ! {
    extlink, _ := peek(c, "extlink").(bool)
    return extlink, nil
    /*return stops.onStack('extlink');*/
  }
    // this must be a word boundary, so previous character must be non-word
    ! {return true, nil /*return /\w/.test(input[endOffset() - 1] || '');*/ }
  r:(
      // urllink, inlined
      target:autourl {
      return target, nil
      /*
        var res = [new SelfclosingTagTk('urllink', [new KV('href', target)], { tsr: tsrOffsets() })];
          return res;
          */
      }
    / autoref
    / isbn) {return r, nil /*return r;*/ }

extlink
  <- ! {
      extlink, _ := peek(c, "extlink").(bool)
      return extlink, nil
      /* return stops.onStack('extlink'); */
    } // extlink cannot be nested
        "["
        # {push(c, "extlink", true); return nil /*return stops.push('extlink', true);*/ }
        addr:(url_protocol urladdr / "")
        target:(extlink_preprocessor_text / "")
        & {
          // TODO: smarter check
          return true, nil
        /*
          // Protocol must be valid and there ought to be at least one
          // post-protocol character.  So strip last char off target
          // before testing protocol.
          var flat = tu.flattenString([addr, target]);
          if (Array.isArray(flat)) {
          // There are templates present, alas.
             return flat.length > 0;
          }
          return Util.isProtocolValid(flat.slice(0, -1), env);
          */
        }
        ( space / unispace )*
        //( "" {return nil, nil /*return endOffset();*/ })
        content:inlineline?
        "]"
        #{ pop(c, "extlink"); return nil }
        {
        n := &html.Node{
          Type: html.ElementNode,
          Data: "a",
          Attr: []html.Attribute{
            {Key: "href", Val: concat(addr, target)},
            {Key: "class", Val: "external"},
            {Key: "rel", Val: "nofollow"},
          },
        }
        addChild(n, content)
        return n, nil
        /*
            stops.pop('extlink');
            return [
                new SelfclosingTagTk('extlink', [
                    new KV('href', tu.flattenString([addr, target])),
                    new KV('mw:content', content || ''),
                    new KV('spaces', sp),
                ], {
                    targetOff: targetOff,
                    tsr: tsrOffsets(),
                    contentOffsets: [targetOff, endOffset() - 1],
                }),
            ];
            */
        }

autoref
  <- ("RFC" / "PMID") space_or_nbsp+ [0-9]+ end_of_word
{ return nil, nil
/*
    var base_urls = {
      'RFC': 'https://tools.ietf.org/html/rfc%s',
      'PMID': '//www.ncbi.nlm.nih.gov/pubmed/%s?dopt=Abstract',
    };
    return [
        new SelfclosingTagTk('extlink', [
           new KV('href', tu.sprintf(base_urls[ref], identifier)),
           new KV('mw:content', tu.flattenString([ref, sp, identifier])),
           new KV('typeof', 'mw:ExtLink/' + ref),
        ],
        { stx: "magiclink", tsr: tsrOffsets() }),
    ];
    */
}

isbn
  <- "ISBN" space_or_nbsp+ (
      [0-9]
      (space_or_nbsp_or_dash &[0-9] {return nil, nil/* return s; */} / [0-9])+
      ((space_or_nbsp_or_dash / "") [xX] / "")
    ) (
      end_of_word
      {return nil, nil
      /*
// Convert isbn token-and-entity array to stripped string.
        return tu.flattenStringlist(isbn).filter(function(e) {
          return e.constructor === String;
        }).join('').replace(/[^\dX]/ig, '').toUpperCase();
        */
      }
    ) &{
    return false, nil
    /*
// ISBNs can only be 10 or 13 digits long (with a specific format)
       return isbncode.length === 10 ||
             (isbncode.length === 13 && /^97[89]/.test(isbncode));
             */
    } {return nil, nil
    /*
      return [
        new SelfclosingTagTk('extlink', [
           new KV('href', 'Special:BookSources/' + isbncode),
           new KV('mw:content', tu.flattenString(['ISBN', sp, isbn])),
           new KV('typeof', 'mw:WikiLink/ISBN'),
        ],
        { stx: "magiclink", tsr: tsrOffsets() }),
      ];
      */
}


// Default URL protocols in MediaWiki (see DefaultSettings). Normally
// these can be configured dynamically. */

url_protocol <-
    & {return false, nil/* return Util.isProtocolValid(input.substr(endOffset()), env); */}
    ( "//" / [A-Za-z] [-A-Za-z0-9+.]* ":" "//"? ) {return nil, nil/* return p;*/ }

// no punctuation, and '{<' to trigger directives
no_punctuation_char <- [^ :\][\r\n"'<>,.&%{]
//TODO:  no_punctuation_char <- [^ :\]\[\r\n"'<>\x00-\x20\x7f,.&%\u00A0\u1680\u180E\u2000-\u200A\u202F\u205F\u3000{]

// this is the general url rule
// on the PHP side, the path part matches EXT_LINK_URL_CLASS
// which is '[^][<>"\x00-\x20\x7F\p{Zs}]'
// the 's' and 'r' pieces below match the characters in
// EXT_LINK_URL_CLASS which aren't included in no_punctuation_char
url
  <- proto:url_protocol
    addr:(urladdr / "")
    path:(  ( !inline_breaks
              c1:no_punctuation_char
              {return c1, nil /*return c; */}
            )
            / s:[.:,']  {return s, nil/* return s; */}
            / comment
            / tplarg_or_template
            / ! ( "&" ( [lL][tT] / [gG][tT] ) ";" )
                r:(
                    & "&" he:htmlentity {return he, nil/* return he; */}
                  / [&%{]
                ) {return r, nil /*return r;*/ }
         )*
// Must be at least one character after the protocol
         & {return false, nil /*return addr.length > 0 || path.length > 0;*/ }
{return []interface{}{proto, addr, path}, nil
/*
    return tu.flattenString([proto, addr].concat(path));
    */
}

// this is the somewhat-restricted rule used in autolinks
// See Parser::doMagicLinks and Parser.php::makeFreeExternalLink.
// The `path` portion matches EXT_LINK_URL_CLASS, as in the general
// url rule.  As in PHP, we do some fancy fixup to yank out
// trailing punctuation, perhaps including parentheses.
// The 's' and 'r' pieces match the characters in EXT_LINK_URL_CLASS
// which aren't included in no_punctuation_char
autourl
  <- &{return true, nil /*return stops.push('autourl', { sawLParen: false }); */}
    ! "//" // protocol-relative autolinks not allowed (T32269)
    (
    url_protocol
    (urladdr / "")
    (  ( !inline_breaks
              ! "("
              c1:no_punctuation_char
              {return c1, nil/* return c; */}
            )
            / "(" {return "(", nil/* stops.onStack('autourl').sawLParen = true; return "("; */}
            / [.:,]
            / (['] ![']) // single quotes are ok, double quotes are bad
            / comment
            / tplarg_or_template
            / ! ( raw_htmlentity &{return false, nil /* return /^[<>\u00A0]$/.test(rhe); */} )
                r:(
                    & "&" he:htmlentity {return he, nil/* return he; */}
                  / [&%{]
                ) {return r, nil/* return r; */}
         )*
{return "TODO: autourl",nil
/*
// as in Parser.php::makeFreeExternalLink, we're going to
// yank trailing punctuation out of this match.
    var url = tu.flattenStringlist([proto, addr].concat(path));
// only need to look at last element; HTML entities are strip-proof.
    var last = lastItem(url);
    var trim = 0;
    if (last && last.constructor === String) {
      var strip = ',;\\.:!?';
      if (!stops.onStack('autourl').sawLParen) {
        strip += ')';
      }
      strip = new RegExp('[' + JSUtils.escapeRegExp(strip) + ']*$');
      trim = strip.exec(last)[0].length;
      url[url.length - 1] = last.slice(0, last.length - trim);
    }
    url = tu.flattenStringlist(url);
    if (url.length === 1 && url[0].constructor === String && url[0].length <= proto.length) {
      return null; // ensure we haven't stripped everything: T106945
    }
    peg$currPos -= trim;
    stops.pop('autourl');
    return url;
    */
} ) &{return false, nil/* return r !== null; */} {return nil, nil/*return r; */}
    / &{return false, nil /*return stops.pop('autourl');*/ }

// This is extracted from EXT_LINK_ADDR in Parser.php: a simplified
// expression to match an IPv6 address.  The IPv4 address and "at least
// one character of a host name" portions are punted to the `path`
// component of the `autourl` and `url` productions
urladdr
  <- ( "[" [0-9A-Fa-f:.]+ "]" )

// ************************************************************
// Templates, -arguments and wikilinks
// ************************************************************/


// Precedence: template arguments win over templates. See
// http://www.mediawiki.org/wiki/Preprocessor_ABNF#Ideal_precedence
// 4: {{{{·}}}} → {·{{{·}}}·}
// 5: {{{{{·}}}}} → {{·{{{·}}}·}}
// 6: {{{{{{·}}}}}} → {{{·{{{·}}}·}}}
// 7: {{{{{{{·}}}}}}} → {·{{{·{{{·}}}·}}}·}
// This is only if close has > 3 braces; otherwise we just match open
// and close as we find them.
//
tplarg_or_template
  <- &"{{" //&{return false, nil}
//
//// Refuse to recurse beyond `maxDepth` levels. Default in the PHP parser
//// is $wgMaxTemplateDepth = 40; This is to prevent crashing from
//// buggy wikitext with lots of unclosed template calls, as in
//// eswiki/Usuario:C%C3%A1rdenas/PRUEBAS?oldid=651094
//      if (stops.onCount('templatedepth') === undefined ||
//          stops.onCount('templatedepth') < env.conf.parsoid.maxDepth) {
//        return true;
//      } else {
//        return false;
//      }
    t:tplarg_or_template_guarded {return t, nil /*return t;*/ }

tplarg_or_template_guarded
  <- #{inc(c, "templatedepth"); return nil /* return stops.inc('templatedepth');*/ }
    r:( &("{{" &("{{{"+ !'{') tplarg) a:(template/broken_template) {return a, nil /*return a;*/ }
      / a:('{' &("{{{"+ !'{'))? b:tplarg {return concat(a, b), nil /*return [a].concat(b);*/ }
      / a:('{' &("{{" !'{'))? b:template {return concat(a, b), nil /*return [a].concat(b);*/ }
      / a:broken_template {return a, nil /*return a;*/ }
    ) #{
      dec(c, "templatedepth")
      return nil
    } {
    return r, nil
    /*
      stops.dec('templatedepth');
      return r;
      */
    }

tplarg_or_template_or_bust
    <- (tplarg_or_template / .)+

template
  <- #{
      push(c, "level", push(c, "preproc", /*{{*/ "}}"))
      return nil
      /* return stops.push('preproc', / * {{ * /"}}"); */
    }
    t:template_preproc
    #{
      popTo(c, "preproc", pop(c, "level").(int))
      return nil
    }
    {return nil, nil/* stops.popTo('preproc', stopLen); return t; */}

// The PHP preprocessor maintains a single stack of "closing token we
// are currently looking for", with no backtracking.  This means that
// once you see `[[ {{` you are looking only for `}}` -- if that template
// turns out to be broken you will never pop the `}}` and there is no way
// to close the `[[`.  Since the PEG tokenizer in Parsoid uses backtracking
// and parses in a single pass (instead of PHP's split preprocessor/parser)
// we have to be a little more careful when we emulate this behavior.
// If we use a rule like:
//   template = "{{" tplname tplargs* "}}"?
// Then we end up having to reinterpret `tplname tplargs*` as a tlb if it
// turns out we never find the `}}`, which involves a lot of tedious gluing
// tokens back together with fingers crossed we haven't discarded any
// significant newlines/whitespace/etc.  An alternative would be a rule like:
//   broken_template = "{{" tlb
// but again, `template` is used in many different contexts; `tlb` isn't
// necessarily the right one to recursively invoke.  Instead we get the
// broken template off of the PEGjs production stack by returning immediately
// after `{{`, but we leave a "broken token" on top of the preprocessor
// stops stack to indicate we're "still in" the {{ context and shouldn't
// ever inlineBreak for any closing tokens above this one.  For example:
//   [[Foo{{Bar]]
// This will match as:
//   wikilink->text,template->text             --> FAILS looking for }}
//     backtracks, popping "]]" and "}}" off preproc stack
//   wikilink->text,broken_template,text       --> FAILS looking for ]]
//     backtracks, popping "]]" and "broken" off preproc stack
//   broken_wikilink,text,broken_template,text --> OK
//     with ["broken", "broken"] left on the preproc stops stack
// Note that we use stops.popTo() to make sure the preproc stack is
// cleaned up properly during backtracking, even if there were broken-FOO
// productions taken which (deliberately) left elements on the preproc stack.

broken_template
  <- &"{{" #{push(c, "preproc", "broken"); return nil/* return stops.push('preproc', 'broken'); */}
// for broken-template,  deliberately fail to pop the preproc stops stack
    t:"{{"
    #{pop(c, "preproc"); return nil}
    {return t, nil/* return t; */}

template_preproc
  <- "{{" nl_comment_space*
    &{log.Println("template_preproc", c.text); return true, nil}
    target:template_param_value
    &{log.Println("template_param_value", concat(c.text)); spew.Dump(target); return true, nil}
    (nl_comment_space* "|"
                r:(
                    v:nl_comment_space*
                    &("|" / "}}")
                    {return v, nil/* return new KV('', tu.flattenIfArray(v), [p0, p0, p0,
                    p]);*/
                    } // empty argument
                    / template_param
                  ) {return r, nil/* return r; */}
            )*
    nl_comment_space*
    &{log.Println("nl_comment_space"); return true, nil}
    inline_breaks "}}" {return target, nil
    /*
// Insert target as first positional attribute, so that it can be
// generically expanded. The TemplateHandler then needs to shift it out
// again.
      params.unshift(new KV(tu.flattenIfArray(target.tokens), '', target.srcOffsets));
      var obj = new SelfclosingTagTk('template', params, { tsr: tsrOffsets(), src: text() });
      return obj;
      */
    } / ("{{" space_or_newline* "}}")

tplarg
  <- //("" {return nil, nil /*return stops.push('preproc', / * {{ * /"}}"); */})
    t:(tplarg_preproc / &{return false, nil  /*return stops.popTo('preproc', stopLen); */} )
    {return t, nil/* stops.popTo('preproc', stopLen); return t; */}

tplarg_preproc
  <- "{{{"
    //("" {return nil, nil/* return endOffset(); */})
    target:template_param_value?
    params:(nl_comment_space* "|"
                ( ("" {return nil, nil/* return endOffset(); */})
                    nl_comment_space*
                    ("" {return nil, nil/* return endOffset(); */})
                    &("|" / "}}}")
                    {return nil, nil/* return {return nil, nil tokens: v, srcOffsets: [p0, p1] }; */}  // empty argument
                    / template_param_value
                  ) {return nil, nil/* return r; */}
            )*
    nl_comment_space*
    inline_breaks "}}}" {return concat(target, params), nil
    /*
      params = params.map(function(o) {
        var s = o.srcOffsets;
        return new KV('', tu.flattenIfArray(o.tokens), [s[0], s[0], s[0], s[1]]);
      });
      if (target === null) { target = { tokens: '', srcOffsets: [p, p, p, p] }; }
// Insert target as first positional attribute, so that it can be
// generically expanded. The TemplateHandler then needs to shift it out
// again.
      params.unshift(new KV(tu.flattenIfArray(target.tokens), '', target.srcOffsets));
      var obj = new SelfclosingTagTk('templatearg', params, { tsr: tsrOffsets(), src: text() });
      return obj;
      */
    }

template_param
  <- template_param_name
    val:(
        //("" {return nil, nil/* return endOffset(); */})
        optionalSpaceToken
        "="
        //("" {return nil, nil/* return endOffset(); */})
        optionalSpaceToken
        tpv:template_param_value? {return tpv, nil
        /*
            return { kEndPos: kEndPos, vStartPos: vStartPos, value: (tpv && tpv.tokens) || [] };
            */
        }
    )? {return val, nil
    /*
      if (val !== null) {
          if (val.value !== null) {
            return new KV(name, tu.flattenIfArray(val.value), [startOffset(), val.kEndPos, val.vStartPos, endOffset()]);
          } else {
            return new KV(tu.flattenIfArray(name), '', [startOffset(), val.kEndPos, val.vStartPos, endOffset()]);
          }
      } else {
        return new KV('', tu.flattenIfArray(name), [startOffset(), startOffset(), startOffset(), endOffset()]);
      }
      */
    }
// empty parameter
  / & [|}] {return nil, nil
  /*
    return new KV('', '', [startOffset(), startOffset(), startOffset(), endOffset()]);
    */
  }

template_param_name
  <- & {
  push(c, "equal", true)
  return true, nil /*return stops.push('equal', true); */}
    tpt:(template_param_text / &'=' {return "", nil/* return ''; */})
    {
    pop(c, "equal")
    return tpt, nil
    /*
        stops.pop('equal');
        return tpt;
        */
    }

  / & {
    pop(c, "equal")
    return false, nil
    /* return stops.pop('equal'); */
  }

template_param_value
  <- #{ push(c, "equal", false); return nil }
    tpt:template_param_text
    #{ pop(c, "equal"); return nil }
    {
    return tpt, nil
    /*
        stops.pop('equal');
        return { tokens: tpt, srcOffsets: tsrOffsets() };
        */
    }

template_param_text
  <- #{
  push(c, "table", false)
  push(c, "extlink", false)
  push(c, "templateArg", true)
  push(c, "tableCellArg", false)
  inc(c, "template")
  return nil
  /*
  // re-enable tables within template parameters
        stops.push('table', false);
        stops.push('extlink', false);
        stops.push('templateArg', true);
        stops.push('tableCellArg', false);
        return stops.inc('template');
        */
    }
    il:(nested_block / newlineToken)+ #{
      pop(c, "table")
      pop(c, "extlink")
      pop(c, "templateArg")
      pop(c, "tableCellArg")
      dec(c, "template")
      return nil
    }
    {
    return il, nil
    /*
        stops.pop('table');
        stops.pop('extlink');
        stops.pop('templateArg');
        stops.pop('tableCellArg');
        stops.dec('template');
// il is guaranteed to be an array -- so, tu.flattenIfArray will
// always return an array
        var r = tu.flattenIfArray(il);
        if (r.length === 1 && r[0].constructor === String) {
            r = r[0];
        }
        return r;
        */
    }

//// Language converter block markup of language variants: -{ ... }-

// Note that "rightmost opening" precedence rule (see
// https://www.mediawiki.org/wiki/Preprocessor_ABNF ) means
// that neither -{{ nor -{{{ are parsed as a -{ token, although
// -{{{{ is (since {{{ has precedence over {{).

lang_variant_or_tpl
  <- &("-{" &("{{{"+ !'{') tplarg) a:lang_variant {return a, nil/* return a; */}
  / a:('-' &("{{{"+ !'{')) b:tplarg {return concat(a, b), nil /*return [a].concat(b);*/ }
  / a:('-' &("{{" "{{{"* !'{')) b:template {return concat(a, b), nil/* return [a].concat(b); */}
  / &"-{" a:lang_variant {return a, nil /*return a; */}

broken_lang_variant
  <- &{return true, nil  /*return stops.push('preproc', 'broken'); */}
// for broken-lang-variant, deliberately fail to pop the stops stack
    r:"-{" {return r, nil /*return r; */}

lang_variant
  <- ("" {return nil, nil /*return stops.push('preproc', /* -{ * / '}-'); */})
    lv:(lang_variant_preproc / &{return false, nil  /*return stops.popTo('preproc', stopLen); */})
    {return lv, nil /*stops.popTo('preproc', stopLen); return lv; */}
  / broken_lang_variant

lang_variant_preproc
  <- ("-{" {return nil, nil/* return startOffset(); */})
    (
       &{return false, nil /* return env.langConverterEnabled(); */}
       ff:opt_lang_variant_flags {return ff, nil
       /*
// Avoid mutating cached expression results
         ff = Util.clone(ff, true);
// if flags contains 'R', then don't treat ; or : specially inside.
         if (ff.flags) {
           ff.raw = ff.flags.has('R') || ff.flags.has('N');
         } else if (ff.variants) {
           ff.raw = true;
         }
         return ff;
         */
       } /
       &{return false, nil  /*return !env.langConverterEnabled(); */}
       "" {return nil, nil
       /*
// if language converter not enabled, don't try to parse inside.
         return { raw: true };
         */
       }
    )
    (
      &{return false, nil  /*return f.raw; */} lv:lang_variant_text {return lv, nil/* return [{ text: lv }]; */}
      /
      &{return false, nil /* return !f.raw; */} lv:lang_variant_option_list {return lv, nil/* return lv; */}
    )
    inline_breaks
    ("}-" {return nil, nil/* return endOffset(); */}) {return "TODO lang_variant_preproc", nil
    /*

      if (!env.langConverterEnabled()) {
        return [ "-{", ts[0].text.tokens, "}-" ];
      }
      var lvsrc = input.substring(lv0, lv1);
      var attribs = [];

// Do a deep clone since we may be destructively modifying
// (the `t[fld] = name;` below) the result of a cached expression
      ts = Util.clone(ts, true);

      ts.forEach(function(t) {
// move token strings into KV attributes so that they are
// properly expanded by early stages of the token pipeline
        ['text','from','to'].forEach(function(fld) {
          if (t[fld] === undefined) { return; }
          var name = 'mw:lv' + attribs.length;
          attribs.push(new KV(name, t[fld].tokens, t[fld].srcOffsets));
          t[fld] = name;
        });
      });
      return [
        new SelfclosingTagTk(
          'language-variant',
           attribs,
           {return nil, nil
             tsr: [lv0, lv1],
             src: lvsrc,
             flags: f.flags && Array.from(f.flags).sort(),
             variants: f.variants && Array.from(f.variants).sort(),
             original: f.original,
             flagSp: f.sp,
             texts: ts,
           }),
      ];
      */
    }

opt_lang_variant_flags
  <- f:( ff:lang_variant_flags "|" {return ff, nil/* return ff; */} )? {return f, nil
  /*
// Collect & separate flags and variants into a set and ordered list
    var flags = new Set();
    var variants = new Set();
    var flagList = [];
    var flagSpace = [];
    var variantList = [];
    var variantSpace = [];
    var useVariants = false;
    var internalSp = []; // internal whitespace, for round-tripping
    if (f !== null) {
// lang_variant_flags returns arrays in reverse order.
      f.flags.reverse();
      f.sp.reverse();
      var spPtr = 0;
      f.flags.forEach(function(item) {
        if (item.flag) {
          flagSpace.push(f.sp[spPtr++]);
          flags.add(item.flag);
          flagList.push(item.flag);
          flagSpace.push(f.sp[spPtr++]);
        }
        if (item.variant) {
          variantSpace.push(f.sp[spPtr++]);
          variants.add(item.variant);
          variantList.push(item.variant);
          variantSpace.push(f.sp[spPtr++]);
        }
      });
      if (spPtr < f.sp.length) {
// handle space after a trailing semicolon
        flagSpace.push(f.sp[spPtr]);
        variantSpace.push(f.sp[spPtr]);
      }
    }
// Parse flags (this logic is from core/languages/ConverterRule.php
// in the parseFlags() function)
    if (flags.size === 0 && variants.size === 0) {
      flags.add('$S');
    } else if (flags.has('R')) {
      flags = new Set(['R']); // remove other flags
    } else if (flags.has('N')) {
      flags = new Set(['N']); // remove other flags
    } else if (flags.has('-')) {
      flags = new Set(['-']); // remove other flags
    } else if (flags.has('T') && flags.size === 1) {
      flags.add('H');
    } else if (flags.has('H')) {
// Replace A flag, and remove other flags except T and D
      var nf = new Set(['$+', 'H']);
      if (flags.has('T')) { nf.add('T'); }
      if (flags.has('D')) { nf.add('D'); }
      flags = nf;
    } else if (variants.size > 0) {
      useVariants = true;
    } else {
      if (flags.has('A')) {
        flags.add('$+');
        flags.add('$S');
      }
      if (flags.has('D')) {
        flags.delete('$S');
      }
    }
    if (useVariants) {
      return { variants: variants, original: variantList, sp: variantSpace };
    } else {
      return { flags: flags, original: flagList, sp: flagSpace };
    }
    */
  }

lang_variant_flags
  <- (space_or_newline*) lang_variant_flag (space_or_newline*)
    ( ";" lang_variant_flags? )? {return nil, nil
    /*
    var r = more && more[1] ? more[1] : { sp: [], flags: [] };
// Note that sp and flags are in reverse order, since we're using
// right recursion and want to push instead of unshift.
    r.sp.push(sp2.join(''));
    r.sp.push(sp1.join(''));
    r.flags.push(f);
    return r;
    */
  }
  / (space_or_newline*) {return nil, nil
  /*
    return { sp: [ sp.join('') ], flags: [] };
    */
  }

lang_variant_flag
  <- [-+A-Z]           {return nil, nil /*return { flag: f }; */}
  / lang_variant_name {return nil, nil/* return { variant: v }; */}
  / (!space_or_newline !nowiki [^{}|;])+ {return nil, nil/* return { bogus: b.join('') }; /*
  bad flag * /*/}

lang_variant_name // language variant name, like zh, zh-cn, etc.
  <- [a-z] [-a-z]+ {return nil, nil/* return h + t.join(''); */}
// Escaped otherwise-unrepresentable language names
// Primarily for supporting html2html round trips; PHP doesn't support
// using nowikis here (yet!)
  / nowiki_text

lang_variant_option_list
  <- lang_variant_option ( ";" lang_variant_option {return nil, nil/* return oo; */})*
    ( ";" space_or_newline* )? // optional trailing semicolon
    {return nil, nil
    /*
      var r = [ o ].concat(rest);
      if (tr) { r.push({ semi: true, sp: tr[1].join('') }); }
      return r;
      */
    }
  / lang_variant_text {return nil, nil/* return [{ text: lvtext }]; */}

lang_variant_option
  <- (space_or_newline*) lang_variant_name
    (space_or_newline*) ":"
    (space_or_newline*)
    (lang_variant_nowiki / lang_variant_text_no_semi)
    {return nil, nil
    /*
      return {
        twoway: true,
        lang: lang,
        text: lvtext,
        sp: [sp1.join(''), sp2.join(''), sp3.join('')]
      };
      */
    }
  / (space_or_newline*)
    (lang_variant_nowiki / lang_variant_text_no_semi_or_arrow)
    "=>"
    (space_or_newline*) lang_variant_name
    (space_or_newline*) ":"
    (space_or_newline*)
    (lang_variant_nowiki / lang_variant_text_no_semi)
    {return nil, nil
    /*
      return {
        oneway: true,
        from: from,
        lang: lang,
        to: to,
        sp: [sp1.join(''), sp2.join(''), sp3.join(''), sp4.join('')]
      };
      */
    }

// html2wt support: If a language name or conversion string can't be
// represented w/o breaking wikitext, just wrap it in a <nowiki>.
// PHP doesn't support this (yet), but Parsoid does.
lang_variant_nowiki
  <- ("" {return nil, nil/*return startOffset();*/})
    nowiki_text
    ("" {return nil, nil/* return endOffset();*/})
    space_or_newline* {return nil, nil
    /*
  return { tokens: [ n ], srcOffsets: [start, end] };
  */
}

lang_variant_text
  <- ("" {return nil, nil/*return startOffset();*/})
    (inlineline / "|" )*
    ("" {return nil, nil/*return endOffset();*/})
    {return nil, nil/* return { tokens: tokens || [], srcOffsets: [start, end] }; */}

lang_variant_text_no_semi
  <- & {return false, nil/* return stops.push('semicolon', true); */}
    lang_variant_text
    {return nil, nil/* stops.pop('semicolon'); return lvtext; */}
  / & {return false, nil/* return stops.pop('semicolon'); */}

lang_variant_text_no_semi_or_arrow
  <- & {return false, nil/* return stops.push('arrow', true); */}
    lang_variant_text_no_semi {return nil, nil/* stops.pop('arrow'); return lvtext; */}
  / & {return false, nil/* return stops.pop('arrow'); */}

wikilink_content
  <-  (pipe lt:link_text? {
    return lt, nil
    /*
        var maybeContent = new KV('mw:maybeContent', lt, [startPos, endOffset()]);
        maybeContent.vsrc = input.substring(startPos, endOffset());
        return maybeContent;
        */
  })*

wikilink <- wikilink_preproc / broken_wikilink

// `broken-link` (see [[:mw:Preprocessor_ABNF]]), but careful because the
// second bracket could start an extlink.  Deliberately leave entry
// on preproc stack since we haven't seen a double-close bracket.
// (See full explanation above broken_template production.)
broken_wikilink
  <- &"[[" #{
      push(c, "preproc", "broken")
      return nil
      /* return stops.push('preproc', 'broken'); */
    }
    a:("[" (extlink / "["))
    #{ pop(c, "preproc"); return nil }
    {
      return a, nil
      /* return a; */
    }

wikilink_preproc
  <- "[["
    #{ push(c, "preproc", "]]"); return nil }
    target:wikilink_preprocessor_text?
    & { log.Printf("wikilink_preproc %#v", target); return true, nil}
    //("" {return nil, nil/* return endOffset(); */})
    lcs:wikilink_content
    & { log.Printf("wikilink_content %#v %s", lcs, c.text); spew.Dump(target); return true, nil}
    inline_breaks "]]"
    #{ pop(c, "preproc"); return nil }
  {
    targetStr := concat(target)
    if strings.HasPrefix(targetStr, "File:") || strings.HasPrefix(targetStr, "Image:") {
      n := &html.Node{
        Type: html.ElementNode,
        Data: "div",
        Attr: []html.Attribute{
          {Key: "class", Val: "image"},
        },
      }
      link := &html.Node{
        Type: html.ElementNode,
        Data: "a",
        Attr: []html.Attribute{
          {Key: "href", Val: TitleToURL(targetStr)},
        },
      }
      addChild(link, targetStr)
      addChild(n, link)
      children, ok := lcs.([]interface{})
      if ok && len(children) > 0 {
        descDiv := &html.Node{
          Type: html.ElementNode,
          Data: "div",
          Attr: []html.Attribute{
            {Key: "class", Val: "caption"},
          },
        }
        addChild(descDiv, children[len(children)-1])
        addChild(n, descDiv)
      }
      return n, nil
    }
    n := &html.Node{
      Type: html.ElementNode,
      Data: "a",
      Attr: []html.Attribute{
        {Key: "href", Val: TitleToURL(targetStr)},
      },
    }
    if !addChild(n, lcs) {
      addChild(n, targetStr)
    }
    return n, nil
  /*
      var pipeTrick = (lcs.length === 1 && lcs[0].v === null);
      var textTokens = [];
      if (target === null || pipeTrick) {
        textTokens.push("[[");
        if (target) {
          textTokens.push(target);
        }
        lcs.forEach(function(a) {
// a is a mw:maybeContent attribute
          textTokens.push("|");
          if (a.v !== null) { textTokens.push(a.v); }
        });
        textTokens.push("]]");
        return textTokens;
      }
      var obj = new SelfclosingTagTk('wikilink');
      var hrefKV = new KV('href', target);
      hrefKV.vsrc = input.substring(startOffset() + 2, tpos);
// XXX: Point to object with path, revision and input information
// obj.source = input;
      obj.attribs.push(hrefKV);
      obj.attribs = obj.attribs.concat(lcs);
      obj.dataAttribs = {
          tsr: tsrOffsets(),
          src: text(),
      };
      return [obj];
      */
  }

// Tables are allowed inside image captions.
link_text
  <- #{
      // Suppress the flag temporarily in this rule to consume the '=' here.
      push(c, "equal", false)
      push(c, "linkdesc", true)
      return nil
    }
    c1:(  // This group is similar to "block_line" but "list_item"
         // is omitted since `doBlockLevels` happens after
         // `replaceInternalLinks2`, where newlines are stripped.
         (sol (heading / hr / full_table_in_link_caption))
       / urltext
       / (!inline_breaks
          r:( inline_element / '[' text_char+ ']' (&(!']' / "]]")) / . ) {return r, nil}
         )
    )+ #{
      pop(c, "equal")
      pop(c, "linkdesc")
      return nil
    }
    {
      return c1, nil
    }

// Generic quote rule for italic and bold, further processed in a token
// stream transformation in doQuotes. Relies on NlTk tokens being emitted
// for each line of text to balance quotes per line.

// We are not using a simple pair rule here as we need to support mis-nested
// bolds/italics and MediaWiki's special heuristics for apostrophes, which are
// all not context free. */
quote <- ("''" "'"*) {return nil, nil
/*
// sequences of four or more than five quotes are assumed to start
// with some number of plain-text apostrophes.
    var plainticks = 0;
    var result = [];
    if (quotes.length === 4) {
        plainticks = 1;
    } else if (quotes.length > 5) {
        plainticks = quotes.length - 5;
    }
    if (plainticks > 0) {
        result.push(quotes.substring(0, plainticks));
    }
// mw-quote token Will be consumed in token transforms
    var tsr = tsrOffsets();
    tsr[0] += plainticks;
    var mwq = new SelfclosingTagTk('mw-quote', [], { tsr: tsr });
    mwq.value = quotes.substring(plainticks);
    result.push(mwq);
    return result;
    */
}


// *********************************************************
// Pre and xmlish tags
// *********************************************************/

extension_tag <-
  &{return false, nil  /*return !stops.onStack('extTag'); */}
  xmlish_tag
// Account for `maybeExtensionTag` returning unmatched start / end tags
  &{return false, nil /* return extToken.name === 'extension'; */}
  {return nil, nil/* return extToken; */}

nowiki
  <- extension_tag
    &{return false, nil /* return extToken.getAttribute('name') === 'nowiki'; */}
    {return nil, nil/* return extToken; */}

// Used by nowiki extension to tokenize html entities.
nowiki_content
  <- c2:(htmlentity / .)* {return c2, nil/* return tu.flattenIfArray(c); */}

// Used by lang_variant productions to protect special language names or
// conversion strings.
nowiki_text
  <- nowiki
  {return nil, nil
  /*
    var txt = Util.getExtArgInfo(extToken).dict.body.extsrc;
    return Util.decodeEntities(txt);
    */
  }

// Generic XML-like tags

// These also cover extensions (including Cite), which will hook into the
// token stream for further processing. The content of extension tags is
// parsed as regular inline, but the source positions of the tag are added
// to allow reconstructing the unparsed text from the input. */

// See http://www.w3.org/TR/html5/syntax.html#tag-open-state and
// following paragraphs.
tag_name_chars <- [^\t\n\v />\x00]
tag_name <- ([A-Za-z] tag_name_chars*)

xmlish_tag
  <- & {return false, nil}
// By the time we get to `doTableStuff` in the php parser, we've already
// safely encoded element attributes. See 55313f4e in core.
//      stops.push('table', false);
//      stops.push('tableCellArg', false);
      //return true;
    //}
    "<" "/"?
    (tag_name & {return false, nil}
    ///*
    //  return isXMLTag(tn, false);  // NOTE: 'extTag' stop was pushed.
    //  */
    //}
    )
    generic_newline_attributes
    space_or_newline* // No need to preserve this -- canonicalize on RT via dirty diff
    "/"?
    space* // not preserved - canonicalized on RT via dirty diff
    ">" {return nil, nil
    /*
        stops.pop('table');
        stops.pop('tableCellArg');
        stops.pop('extTag');

        var lcName = name.toLowerCase();

// Extension tags don't necessarily have the same semantics as html tags,
// so don't treat them as void elements.
        var isVoidElt = Util.isVoidElement(lcName) && !env.conf.wiki.extensionTags.has(lcName);

// Support </br>
        if (lcName === 'br' && end) {
            end = null;
        }

        var res = tu.buildXMLTag(name, lcName, attribs, end, !!selfclose || isVoidElt, tsrOffsets());

// change up data-attribs in one scenario
// void-elts that aren't self-closed ==> useful for accurate RT-ing
        if (!selfclose && isVoidElt) {
            res.dataAttribs.selfClose = undefined;
            res.dataAttribs.noClose = true;
        }

        return maybeExtensionTag(res);
        */
    }
    / "<" "/"? tag_name & {return false, nil /* return stops.pop('extTag'); */}
    / & {return false, nil /* stops.pop('table'); return stops.pop('tableCellArg'); */}


// A variant of xmlish_tag, but also checks if the tag name is a block-level
// tag as defined in
// http://www.w3.org/TR/html5/syntax.html#tag-open-state and
// following paragraphs.
//
block_tag
  <- & {
      // By the time we get to `doTableStuff` in the php parser, we've already
      // safely encoded element attributes. See 55313f4e in core.
      push(c, "table", false)
      push(c, "tableCellArg", false)
      return true, nil
    }
    "<" "/"?
    (tag_name & {
      push(c, "extTag", false)
      return false, nil
    }
    //#/*
    //#  return isXMLTag(tn, true);  // NOTE: 'extTag' stop was pushed.
    //#  */
    //#}
    )
    generic_newline_attributes
    space_or_newline*
    "/"?
    ">" {
      pop(c, "table")
      pop(c, "tableCellArg")
      pop(c, "extTag")
      return nil, nil
    /*
      stops.pop('table');
      stops.pop('tableCellArg');
      stops.pop('extTag');
      var t = tu.buildXMLTag(name, name.toLowerCase(), attribs, end, !!selfclose, tsrOffsets());
      return [maybeExtensionTag(t)];
      */
    }
    / "<" "/"? tag_name & {
      pop(c, "extTag")
      return false, nil
    }
    / & {
      pop(c, "table")
      pop(c, "tableCellArg")
      return false, nil
    }

// A generic attribute that can span multiple lines.
generic_newline_attribute
  <- space_or_newline*
    ("" {return nil, nil/* return endOffset(); */})
    generic_attribute_name
    ("" {return nil, nil/* return endOffset(); */})
    (space_or_newline* "=" generic_att_value? {return nil, nil/* return v; */})?
{return nil,nil
/*
// NB: Keep in sync w/ table_attibute
    var res;
// Encapsulate protected attributes.
    if (typeof name === 'string') {return nil, nil
        name = tu.protectAttrs(name);
    }
    if (vd !== null) {
        res = new KV(name, vd.value, [namePos0, namePos, vd.srcOffsets[0], vd.srcOffsets[1]]);
        res.vsrc = input.substring(vd.srcOffsets[0], vd.srcOffsets[1]);
    } else {
        res = new KV(name, '', [namePos0, namePos, namePos, namePos]);
    }
    if (Array.isArray(name)) {
        res.ksrc = input.substring(namePos0, namePos);
    }
    return res;
    */
}

// A single-line attribute.
table_attribute
  <- optionalSpaceToken
    ("" {return nil, nil /* return endOffset(); */})
    table_attribute_name
    ("" {return nil, nil /* return endOffset(); */})
    (optionalSpaceToken "=" table_att_value? {return nil, nil /* return v; */})?
{return nil,nil
/*
// NB: Keep in sync w/ generic_newline_attribute
    var res;
// Encapsulate protected attributes.
    if (typeof name === 'string') {
        name = tu.protectAttrs(name);
    }
    if (vd !== null) {
        res = new KV(name, vd.value, [namePos0, namePos, vd.srcOffsets[0], vd.srcOffsets[1]]);
        res.vsrc = input.substring(vd.srcOffsets[0], vd.srcOffsets[1]);
    } else {
        res = new KV(name, '', [namePos0, namePos, namePos, namePos]);
    }
    if (Array.isArray(name)) {
        res.ksrc = input.substring(namePos0, namePos);
    }
    return res;
    */
}

// The arrangement of chars is to emphasize the split between what's disallowed
// by html5 and what's necessary to give directive a chance.
// See: http://www.w3.org/TR/html5/syntax.html#attributes-0
generic_attribute_name
  <- (["'=]?)  // From #before-attribute-name-state, < is omitted for directive
    ( [^ \t\r\n\x00/=><&{}!|-]+
        / !inline_breaks
// \0/=> is the html5 attribute name set we do not want.
          ( directive / !( space_or_newline / [\x00/=>] ) { return nil, nil /*return c;*/ }
        ) {return nil, nil  /*return t; */}
    )*
    & {return false, nil/* return r.length > 0 || q.length > 0; */}
  {return nil, nil /* return tu.flattenString([q].concat(r)); */}

// Also accept these chars in a wikitext table or tr attribute name position.
// They are normally not matched by the table_attribute_name.
broken_table_attribute_name_char <- [\x00/=>] {return nil, nil /* return new KV(c, ''); */}

// Same as generic_attribute_name, except for accepting tags and wikilinks.
// (That doesn't make sense (ie. match php) in the generic case.)
// We also give a chance to break on \[ (see T2553).
table_attribute_name
  <- (["'=]?)  // From #before-attribute-name-state, < is omitted for directive
    ( [^ \t\r\n\x00/=><&{}!|[-]+
        / !inline_breaks
// \0/=> is the html5 attribute name set we do not want.
          (   wikilink
              / directive
// Accept insane tags-inside-attributes as attribute names.
// The sanitizer will strip and shadow them for roundtripping.
// Example: <hiddentext>generated with.. </hiddentext>
              / &xmlish_tag inlineline {return nil, nil/* return ill; */}
              / !( space_or_newline / [\x00/=>] ) . {return nil, nil/* return c; */}
        ) {return nil, nil/* return t; */}
    )*
    & {return false, nil/* return r.length > 0 || q.length > 0; */}
  {return nil, nil/* return tu.flattenString([q].concat(r)); */}

// Attribute value, quoted variants can span multiple lines.
// Missing end quote: accept /> look-ahead as heuristic.
// These need to be kept in sync with the attribute_preprocessor_text_*
generic_att_value
  <- (space_or_newline* "'") attribute_preprocessor_text_single? ("'" / &('/'? '>')) {return nil, nil
  /*
      return tu.getAttrVal(t, startOffset() + s.length, endOffset() - q.length);
      */
    }
  / (space_or_newline* '"') attribute_preprocessor_text_double? ('"' / &('/'? '>')) {return nil, nil
  /*
      return tu.getAttrVal(t, startOffset() + s.length, endOffset() - q.length);
      */
    }
  / space_or_newline* attribute_preprocessor_text &(space_or_newline / eof / '/'? '>') {return nil, nil
  /*
      return tu.getAttrVal(t, startOffset() + s.length, endOffset());
      */
    }

// Attribute value, restricted to a single line.
// Missing end quote: accept |, !!, \r, and \n look-ahead as heuristic.
// These need to be kept in sync with the table_attribute_preprocessor_text_*
table_att_value
  <- (space* "'") table_attribute_preprocessor_text_single? ("'" / &("!!" / [|\r\n])) {return nil, nil
  /*
      return tu.getAttrVal(t, startOffset() + s.length, endOffset() - q.length);
      */
    }
  / (space* '"') table_attribute_preprocessor_text_double? ('"' / &("!!" / [|\r\n])) {return nil, nil
  /*
      return tu.getAttrVal(t, startOffset() + s.length, endOffset() - q.length);
      */
    }
  / space* table_attribute_preprocessor_text &(space_or_newline/ eof / "!!" / '|') {return nil, nil
  /*
      return tu.getAttrVal(t, startOffset() + s.length, endOffset());
      */
    }

// *******************************************************
//   Lists
// *******************************************************/
list_item <- dtdd / hacky_dl_uses / li

li <- bullets:list_char+
     c2:inlineline?
     // The inline_break is to check if we've hit a template end delimiter.
     &(eolf / inline_breaks)
{
  n := &html.Node{
    Type: html.ElementNode,
    Data: "li",
  }
  addChild(n, c2)
  return n ,nil
/*
// Leave bullets as an array -- list handler expects this
    var tsr = tsrOffsets('start');
    tsr[1] += bullets.length;
    var li = new TagTk('listItem', [], { tsr: tsr });
    li.bullets = bullets;
    return [ li ].concat(c || []);
    */
}


// This rule is required to support wikitext of this form
//   ::{|border="1"|foo|bar|baz|}
// where the leading colons are used to indent the entire table.
// This hack was added back in 2006 in commit
// a0746946312b0f1eda30a2c793f5f7052e8e5f3a based on a patch by Carl
// Fürstenberg.
//
hacky_dl_uses <- ":"+
               (table_line (sol table_line)*)
               inlineline?
               &comment_space_eolf
{return nil,nil
/*
// Leave bullets as an array -- list handler expects this
    var tsr = tsrOffsets('start');
    tsr[1] += bullets.length;
    var li = new TagTk('listItem', [], { tsr: tsr });
    li.bullets = bullets;
    return tu.flattenIfArray([li, tbl || [], line || []]);
    */
}

dtdd
  <- (!(";" !list_char) list_char {return nil, nil /*return lc;*/ })*
    ";"
    & {return false, nil/*return stops.inc('colon');*/}
    inlineline?
    (":" {return nil, nil /*return endOffset(); */})
// Fortunately dtdds cannot be nested, so we can simply set the flag
// back to 0 to disable it.
    & {return false, nil /*stops.counters.colon = 0; return true;*/}
    inlineline?
    &eolf {return nil, nil
    /*
// Leave bullets as an array -- list handler expects this
// TSR: +1 for the leading ";"
        var numBullets = bullets.length + 1;
        var tsr = tsrOffsets('start');
        tsr[1] += numBullets;
        var li1 = new TagTk('listItem', [], { tsr: tsr });
        li1.bullets = bullets.slice();
        li1.bullets.push(";");
// TSR: -1 for the intermediate ":"
        var li2 = new TagTk('listItem', [], { tsr: [cpos - 1, cpos], stx: 'row' });
        li2.bullets = bullets.slice();
        li2.bullets.push(":");

        return [ li1 ].concat(c || [], [ li2 ], d || []);
        */
    }
// Fall-back case to clear the colon flag
  / & {return false, nil /*stops.counters.colon = 0; return false; */}


list_char <- [*#:;]



// ****************************************************************************
// Tables
// ------
// Table rules are geared to support independent parsing of fragments in
// templates (the common table start / row / table end use case). The tokens
// produced by these fragments then match up to a table while building the
// DOM tree. For similar reasons, table rows do not emit explicit end tag
// tokens.

// The separate table_line rule is faster than moving those rules
// directly to block_lines.

// Notes about the full_table_in_link_caption rule
// -----------------------------------------------------
// However, for link-tables, we have introduced a stricter parse wherein
// we require table-start and table-end tags to not come from a template.
// In addition, this new rule doesn't accept fosterable-content in
// the table unlike the more lax (sol table_line)+ rule.

// This is the best we can do at this time since we cannot distinguish
// between table rows and image options entirely in the tokenizer.

// Consider the following examples:

// Example 1:

// [[Image:Foo.jpg|left|30px|Example 1
// {{This-template-returns-a-table-start-tag}}
// |foo
// {{This-template-returns-a-table-end-tag}}
// ]]

// Example 2:

// [[Image:Foo.jpg|left|30px|Example 1
// {{echo|a}}
// |foo
// {{echo|b}}
// ]]

// So, we cannot know a priori (without preprocessing or fully expanding
// all templates) if "|foo" in the two examples is a table cell or an image
// option. This is a limitation of our tokenizer-based approach compared to
// the preprocessing-based approach of the PHP parser.

// Given this limitation, we are okay forcing a full-table context in
// link captions (if necessary, we can relax the fosterable-content requirement
// but that is broken wikitext anyway, so we can force that edge-case wikitext
// to get fixed by rejecting it).
// ****************************************************************************/

full_table_in_link_caption
  <- (! inline_breaks / & "{{!}}" )
    (
// Note that "linkdesc" is suppressed here to provide a nested parsing
// context in which to parse the table.  Otherwise, we may break on
// on pipes in the `table_start_tag` and `table_row_tag` attributes.
// However, as a result, this can be more permissive than the current
// php implementation, but likelier to match the users intent.
        & {return false, nil /*stops.push('linkdesc', false); return stops.push('table', true);
        */}
        (
            table_start_tag optionalNewlines
// Accept multiple end tags since a nested table may have been
// opened in the table content line.
            ((sol (table_content_line / tplarg_or_template) optionalNewlines)*
            sol table_end_tag)+
        ){return nil, nil
        /*
            stops.pop('linkdesc');
            stops.pop('table');
            return tbl;
            */
        }
      / & {return false, nil/* stops.pop('linkdesc'); return stops.pop('table'); */}
    ) {return nil, nil/* return r; */}

// This rule assumes start-of-line position!
table_line
  <- (! inline_breaks / & "{{!}}" )
    (
        & {return false, nil /* return stops.push('table', true); */}
        (
             table_start_tag optionalNewlines
           / table_content_line optionalNewlines
           / table_end_tag
        ) {return nil, nil
        /*
            stops.pop('table');
            return tl;
            */
        }
      / & {return false, nil /* return stops.pop('table'); */}
    ) {return nil, nil/* return r; */}

table_content_line <- (space / comment)* (
    table_heading_tags
    / table_row_tag
    / table_data_tags
    / table_caption_tag
  )

table_start_tag
  <- (space / comment)* ("" {return nil, nil/* return endOffset(); */}) "{" pipe
// ok to normalize away stray |} on rt (see T59360)
    & {return false, nil /* return stops.push('table', false); */}
    table_attributes
    ("" {return nil, nil/* stops.pop('table'); return endOffset(); */})
    {return nil, nil
    /*
        var coms = tu.popComments(ta);
        if (coms) {
          tsEndPos = coms.commentStartPos;
        }

        var da = { tsr: [startPos, tsEndPos] };
        if (p !== "|") {
// Variation from default
            da.startTagSrc = b + p;
        }

        sc.push(new TagTk('table', ta, da));
        if (coms) {
          sc = sc.concat(coms.buf);
        }
        return sc;
        */
    }

// FIXME: Not sure if we want to support it, but this should allow columns.
table_caption_tag
// avoid recursion via nested_block_in_table
  <- ! {return true, nil /*return stops.onStack('tableDataBlock');*/ }
    pipe "+"
    row_syntax_table_args?
    ("" {return nil, nil /*return endOffset();*/ })
    nested_block_in_table* {return nil, nil
    /*
        return tu.buildTableTokens("caption", "|+", args, [startOffset(), tagEndPos], endOffset(), c, true);
        */
    }

table_row_tag
  <- // avoid recursion via nested_block_in_table
    ! {return true, nil /*return stops.onStack('tableDataBlock'); */}
    pipe "-"+
    & {return false, nil /* return stops.push('table', false); */}
    table_attributes
    ("" {return nil, nil/* stops.pop('table'); return endOffset(); */})
    {return nil, nil
    /*
        var coms = tu.popComments(a);
        if (coms) {
          tagEndPos = coms.commentStartPos;
        }

        var da = {
          tsr: [ startOffset(), tagEndPos ],
          startTagSrc: p + dashes,
        };

// We rely on our tree builder to close the row as needed. This is
// needed to support building tables from fragment templates with
// individual cells or rows.
        var trToken = new TagTk('tr', a, da);

        var res = [ trToken ];
        if (coms) {
          res = res.concat(coms.buf);
        }
        return res;
        */
    }

tds
  <- ( ( pipe_pipe / pipe & row_syntax_table_args {return nil, nil /*return p;*/ } )
      table_data_tag {return nil, nil
      /*
        var da = tdt[0].dataAttribs;
        da.stx = "row";
        da.tsr[0] -= pp.length; // include "||"
        if (pp !== "||" || (da.startTagSrc && da.startTagSrc !== pp)) {
// Variation from default
          da.startTagSrc = pp + (da.startTagSrc ? da.startTagSrc.substring(1) : '');
        }
        return tdt;
        */
      }
    )*

// avoid recursion via nested_block_in_table
table_data_tags
  <- ! {return true, nil/* return stops.onStack('tableDataBlock'); */}
    pipe
    ![+-] table_data_tag
    ("" {return nil, nil/* return endOffset(); */})
    tds {return nil, nil
    // blahaskjdf;alsdf;;
    }

table_data_tag
  <- ! "}"
    row_syntax_table_args?
// use inline_breaks to break on tr etc
    ("" {return nil, nil/* return endOffset(); */})
    nested_block_in_table*
    {return nil, nil
    /*
        return tu.buildTableTokens("td", "|", arg, [startOffset(), tagEndPos], endOffset(), td);
        */
    }

table_heading_tags
  <- "!"
    & {return false, nil /*return stops.push('th', endOffset()); */}
    table_heading_tag
    ( ("!!" / pipe_pipe) table_heading_tag {return nil, nil
    /*
            var da = tht[0].dataAttribs;
            da.stx = 'row';
            da.tsr[0] -= pp.length; // include "!!" or "||"

            if (pp !== "!!" || (da.startTagSrc && da.startTagSrc !== pp)) {
// Variation from default
                da.startTagSrc = pp + (da.startTagSrc ? da.startTagSrc.substring(1) : '');
            }
            return tht;
            */
          }
    )* {return nil, nil
    /*
        stops.pop('th');
        th[0].dataAttribs.tsr[0]--; // include "!"
        return th.concat(ths);
        */
    }
    / & {return false, nil /*return stops.onStack('th') !== false ? stops.pop('th') : false;*/ }

table_heading_tag
  <- row_syntax_table_args?
    ("" {return nil, nil /*return endOffset();*/ })
    ( & {return false, nil
    /*
// This SyntaxStop is only true until we hit the end of the line.
      if (stops.onStack('th') !== false &&
              /\n/.test(input.substring(stops.onStack('th'), endOffset()))) {
// There's been a newline. Remove the break and continue
// tokenizing nested_block_in_tables.
          stops.pop('th');
      }
      return true;
      */
    } nested_block_in_table {return nil, nil/* return d; */} )* {return nil, nil
    /*
        return tu.buildTableTokens("th", "!", arg, [startOffset(), tagEndPos], endOffset(), c);
        */
    }

table_end_tag
  <- (space / comment)* ("" {return nil, nil/* return endOffset(); */}) pipe "}" {return nil, nil
  /*
      var tblEnd = new EndTagTk('table', [], { tsr: [startPos, endOffset()] });
      if (p !== "|") {
// p+"<brace-char>" is triggering some bug in pegJS
// I cannot even use that expression in the comment!
          tblEnd.dataAttribs.endTagSrc = p + b;
      }
      return sc.concat([tblEnd]);
      */
  }

//
// Table parameters separated from the content by a single pipe. Does *not*
// match if followed by double pipe (row-based syntax).
//
row_syntax_table_args
  <- & {return false, nil /* return stops.push('tableCellArg', return true, nil); */}
    table_attributes space* pipe !pipe {return nil, nil
    /*
        stops.pop('tableCellArg');
        return [as, s, p];
        */
    }
    / & {return false, nil /* return stops.pop('tableCellArg'); */}


// *****************************************************************
// Text variants and other general rules
// *****************************************************************/

// All chars that cannot start syntactic structures in the middle of a line
// XXX: ] and other end delimiters should probably only be activated inside
// structures to avoid unnecessarily leaving the text rule on plain
// content.

// TODO: Much of this is should really be context-dependent (syntactic
// flags). The wikilink_preprocessor_text rule is an example where
// text_char is not quite right and had to be augmented. Try to minimize /
// clarify this carefully!
//

text_char <- [^'<~[{\n\r:;\]}|!=-]

// Legend
// '    quotes (italic/bold)
// <    start of xmlish_tag
// ~    signatures/dates
// [    start of links
// {    start of parser functions, transclusion and template args
// \n   all sort of block-level markup at start of line
// \r   ditto
// A-Za-z autolinks (http(s), nttp(s), mailto, ISBN, PMID, RFC)

// _    behavior switches (e.g., '__NOTOC__') (XXX: not URL related)
// ! and | table cell delimiters, might be better to specialize those
// =    headings - also specialize those!

// The following chars are also included for now, but only apply in some
// contexts and should probably be enabled only in those:
// :    separate definition in ; term : definition
// ]    end of link
// }    end of parser func/transclusion/template arg
// -    start of lang_variant -{ ... }-
// ;    separator in lang_variant
//

urltext <- ( [^-'<~[{\n/A-Za-z_|!:;\]} &=]+
          / & [/A-Za-z] al:autolink {return al, nil /*return al;*/ }
          / & "&" he:htmlentity {return he, nil /*return he;*/ }
// Convert trailing space into &nbsp;
// XXX: This should be moved to a serializer
// This is a hack to force a whitespace display before the colon
          / ' ' & ':' {return "&nbsp;", nil
          /*
              var toks = Util.placeholder('\u00a0', {
                 ' ',
                tsr: tsrOffsets('start'),
                isDisplayHack: true,
              }, { tsr: tsrOffsets('end'), isDisplayHack: true });
              var typeOf = toks[0].getAttribute('typeof');
              toks[0].setAttribute('typeof', 'mw:DisplaySpace ' + typeOf);
              return toks;
              */
          }
          / & ("__") bs:behavior_switch {return bs, nil /*return bs;*/ }
// About 96% of text_char calls originate here.
// pegjs 0.8 inlines this simple rule automatically.
          / text_char )+

raw_htmlentity <- ("&" [#0-9a-zA-Z]+ ";") {return nil, nil
/*
    return Util.decodeEntities(m);
    */
}

htmlentity <- raw_htmlentity {return nil, nil
/*
// if this is an invalid entity, don't tag it with 'mw:Entity'
    if (cc.length > 2 /* decoded entity would be 1 or 2 UTF-16 characters * /) {
        return cc;
    }
    return [
        new TagTk('span', [new KV('typeof', 'mw:Entity')], { src: text(), srcContent: cc, tsr: tsrOffsets('start') }),
        cc,
        new EndTagTk('span', [], { tsr: tsrOffsets('end') }),
    ];
    */
}

spaces <- [ \t]+

space <- [ \t]

optionalSpaceToken <- space*

// This rule corresponds to \s in the PHP preg_* functions,
// which is used frequently in the PHP parser.  The inclusion of
// form feed (but not other whitespace, like vertical tab) is a quirk
// of Perl, which PHP inherited via the PCRE (Perl-Compatible Regular
// Expressions) library.
//
space_or_newline
  <- [ \t\n\r\x0c]

// This rule corresponds to \b in the PHP preg_* functions,
// after a word character.  That is, it's a zero-width lookahead that
// the next character is not a word character.
//
end_of_word
  <- eof / ![A-Za-z0-9_]

// Unicode "separator, space" category.  It covers the \u0020 space as well
// as \u3000 IDEOGRAPHIC SPACE (see bug 19052).  In PHP this is \p{Zs}.
// Keep this up-to-date with the characters tagged ;Zs; in
// http://www.unicode.org/Public/UNIDATA/UnicodeData.txt
unispace <- [ \u00A0\u1680\u2000-\u200A\u202F\u205F\u3000]

// Non-newline whitespace, including non-breaking spaces.  Used for magic links.
space_or_nbsp
  <- space // includes \t
  / unispace
  / he:htmlentity &{ return false, nil /*return Array.isArray(he) && /^\u00A0$/.test(he[1]);*/ }
    {return he, nil /*return he;*/ }

// Used within ISBN magic links
space_or_nbsp_or_dash
  <- space_or_nbsp / "-"

// Extra newlines followed by at least another newline. Usually used to
// compress surplus newlines into a meta tag, so that they don't trigger
// paragraphs.
optionalNewlines
  <- ([\n\r\t ] &[\n\r])*

comment_or_includes <- (comment / (
    ( #{
      push(c, "sol_il", true)
      return nil
    }
      i:include_limits
      #{
        pop(c, "sol_il")
        return nil
      }
    ) {return i, nil}
  ))*

sol <- (empty_line_with_comments / sol_prefix) comment_or_includes

sol_prefix
  <- newlineToken
  / & {
  //log.Printf("sol_prefix %v", c.pos)
  return c.pos.offset == 0, nil
  /*
// Use the sol flag only at the start of the input
// NOTE: Explicitly check for 'false' and not a falsy value
      return endOffset() === 0 && options.sol !== false;
      */
  } {return nil, nil /*return [];*/ }

empty_line_with_comments
  <- sol_prefix ("" {return "empty_line_with_comments", nil /*return endOffset();*/ }) (space* comment (space / comment)* newline)+ {return nil, nil
  /*
        return [
            sp,
            new SelfclosingTagTk("meta", [new KV('typeof', 'mw:EmptyLine')], {
                tokens: tu.flattenIfArray(c),
                tsr: [p, endOffset()],
            }),
        ];
        */
    }

comment_space <- comment / space

nl_comment_space <- newlineToken / comment_space

//
// noinclude / includeonly / onlyinclude rules. These are normally
// handled by the xmlish_tag rule, except where generic tags are not
// allowed- for example in directives, which are allowed in various attribute
// names and -values.

// Example test case:
// {|
// |-<includeonly>
// foo
// </includeonly>
// |Hello
// |}
//

include_limits <-
  il:("<" "/"? ([oyinclude]i+ & {return false, nil
  /*
    var incl = n.toLowerCase();
    return incl === "noinclude" || incl === "onlyinclude" ||
      incl === "includeonly";
      */
  }) space_or_newline* ">" {return nil, nil
  /*
    var incl = name.toLowerCase();
    var dp = { tsr: tsrOffsets() };

// Record variant since tag is not in normalized lower case
    if (name !== incl) {
      dp.srcTagName = name;
    }

// End tag only
    if (c) {
      return new EndTagTk(name, [], dp);
    }

    var restOfInput = input.substring(endOffset());
    var tagContent = restOfInput.match(new RegExp("^([\\s\\S]*?)(?:</\\s*" + incl + "\\s*>)", "m"));

// Start tag only
    if (!tagContent || !tagContent[1]) {
      return new TagTk(name, [], dp);
    }

// Get the content
    var inclContent = tagContent[1];

// Preserve SOL where necessary (for onlyinclude and noinclude)
// Note that this only works because we encounter <*include*> tags in
// the toplevel content and we rely on the php preprocessor to expand
// templates, so we shouldn't ever be tokenizing inInclude.
// Last line should be empty (except for comments)
    if (incl !== "includeonly" && stops.onStack("sol_il")) {
      var last = lastItem(inclContent.split('\n'));
      if (!/^(<!--([^-]|-(?!->))*-->)*$/.test(last)) {
        return false;
      }
    }

// Tokenize include content in a new tokenizer
    var inclContentToks = (new PegTokenizer(env)).tokenizeSync(inclContent);
    inclContentToks = Util.stripEOFTkfromTokens(inclContentToks);

// Shift tsr
    Util.shiftTokenTSR(inclContentToks, endOffset());

// Skip past content
    peg$currPos += inclContent.length;

    return [new TagTk(name, [], dp)].concat(inclContentToks);
      */
  }) & {return il != nil, nil /*return !!il; */ } {return il, nil /*return il; */ }

// Start of file
sof <- & {
  return c.pos.offset == 0, nil
}

// End of file
eof <- & {
  len := c.globalStore["len"].(int)
  return c.pos.offset == len, nil
}

newline <- '\n' / "\r\n"

newlineToken <- newline {return "\n", nil/* return [new NlTk(tsrOffsets())]; */}

eolf <- newline / eof

comment_space_eolf <- (space+ / comment)* eolf

// 'Preprocessor' directive- higher-level things that can occur in otherwise
// plain-text content.
directive
  <- comment
  / extension_tag
  / tplarg_or_template
  / & "-{" v:lang_variant_or_tpl {return v, nil/* return v; */}
  / & "&" e:htmlentity {return e, nil/* return e; */}
  / include_limits

wikilink_preprocessor_text
  <- r:( [^<[{\n\r\t|!\]}{ &-]+
// XXX gwicke: any more chars we need to allow here?
        / !inline_breaks wr:( directive / ( !"]]" ( text_char / [!<}\]\n\r-] ) ) )
        {return wr, nil/* return wr; */}
    )+ {return r, nil
    /*
      return tu.flattenStringlist(r);
      */
  }

extlink_preprocessor_text
// added special separator character class inline: separates url from
// description / text
  <- # { push(c, "linkdesc", false); return nil
  /*
// Prevent breaking on pipes when we're in a link description.
// See the test, 'Images with the "|" character in the comment'.
    return stops.push('linkdesc', false);
    */
  }
  r:( [^'<~[{\n\r|!\]}\t&="' \u00A0\u1680\u180E\u2000-\u200A\u202F\u205F\u3000-]+
  / !inline_breaks s:( directive / no_punctuation_char / [&|{-] ) {return s, nil/* return s;
  */}
/// urlencoded_char
// !inline_breaks no_punctuation_char
  / ([.:,] !(space / eolf))
  / (['] ![']) // single quotes are ok, double quotes are bad
  )+
  #{ pop(c, "linkdesc"); return nil }
  {return r, nil
  /*
      stops.pop('linkdesc');
      return tu.flattenString(r);
      */
  }

// Attribute values with preprocessor support

// n.b. / is a permissible char in the three rules below.
// We only break on />, enforced by the negated expression.
// Hence, it isn't included in the stop set.

// The stop set is space_or_newline and > which matches generic_att_value.
attribute_preprocessor_text
  <- r:( [^{}&<|/ \t\n\r\x0c>-]+
  / !inline_breaks
    !"/>"
    s:( directive / [{}&<|/-] ) {return s, nil /*return s; */}
  )+ {return r, nil
  /*
    return tu.flattenString(r);
    */
  }

// The stop set is '> which matches generic_att_value.
attribute_preprocessor_text_single
  <- r:( [^{}&<|/'>-]+
  / !inline_breaks
    !"/>"
    s:( directive / [{}&<|/-] ) {return s, nil/* return s; */}
  )* {return r, nil
  /*
    return tu.flattenString(r);
    */
  }

// The stop set is "> which matches generic_att_value.
attribute_preprocessor_text_double
  <- r:( [^{}&<|/">-]+
  / !inline_breaks
    !"/>"
    s:( directive / [{}&<|/-] ) {return s, nil/* return s; */}
  )* {return r, nil
  /*
    return tu.flattenString(r);
    */
  }

// Variants with the entire attribute on a single line

// n.b. ! is a permissible char in the three rules below.
// We only break on !! in th, enforced by the inline break.
// Hence, it isn't included in the stop set.
// [ is also permissible but we give a chance to break
// for the [[ special case in php's doTableStuff (See T2553).

// The stop set is space_or_newline and | which matches table_att_value.
table_attribute_preprocessor_text
  <- r:( [^{}&<![ \t\n\r\x0c|-]+
  / !inline_breaks s:( directive / [{}&<![-] ) {return s, nil/* return s; */}
  )+ {return r, nil
  /*
    return tu.flattenString(r);
    */
  }

// The stop set is '\r\n| which matches table_att_value.
table_attribute_preprocessor_text_single
  <- r:( [^{}&<!['\r\n|-]+
  / !inline_breaks s:( directive / [{}&<![-] ) {return s, nil/* return s; */}
  )* {return r, nil
  /*
    return tu.flattenString(r);
    */
  }

// The stop set is "\r\n| which matches table_att_value.
table_attribute_preprocessor_text_double
  <- r:( [^{}&<!["\r\n|-]+
  / !inline_breaks s:( directive / [{}&<![-] ) {return s, nil/* return s; */}
  )* {return r, nil
  /*
    return tu.flattenString(r);
    */
  }

// Special-case support for those pipe templates
pipe <- "|" / "{{!}}"

// SSS FIXME: what about |{{!}} and {{!}}|
pipe_pipe <- "||" / "{{!}}{{!}}"
